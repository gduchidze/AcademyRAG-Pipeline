{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo9F6Ufo4Md2"
      },
      "source": [
        "\n",
        "# შესავალი უკუგავრცელებასა და გრადიენტულ დაშვებაში\n",
        "უკუგავრცელება და გრადიენტული დაშვება არის ორი ფუნდამენტური კონცეფცია მანქანური სწავლების სფეროში, განსაკუთრებით ნეირონული ქსელების ტრენირებისას. ამ ლექციაში ჩვენ გავეცნობით ამ ორ მეთოდს და მათ მნიშვნელობას.\n",
        "\n",
        "\n",
        "გრადიენტული დაშვება არის ოპტიმიზაციის ალგორითმი, რომელიც გამოიყენება ფუნქციის მინიმუმის ან მაქსიმუმის საპოვნელად. მანქანურ სწავლებაში ის გამოიყენება დანაკარგის ფუნქციის მინიმიზაციისთვის.\n",
        "\n",
        "გრადიენტული დაშვების ძირითადი იდეა მდგომარეობს იმაში, რომ ვიპოვოთ ფუნქციის მინიმუმი (ან მაქსიმუმი) მცირე ნაბიჯებით გადაადგილებით გრადიენტის მიმართულებით.\n",
        "\n",
        "მათემატიკურად, გრადიენტული დაშვება შეიძლება გამოვსახოთ შემდეგნაირად:\n",
        "\n",
        "    θ(t+1) = θ(t) - η∇J(θ(t))\n",
        "\n",
        "სადაც:\n",
        "* θ - პარამეტრების ვექტორი\n",
        "* t - იტერაცია\n",
        "* η - სწავლების სიჩქარე (learning rate)\n",
        "* ∇J(θ) - დანაკარგის ფუნქციის გრადიენტი"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXnxpHbf4cbe"
      },
      "outputs": [],
      "source": [
        "# გრადიენტული დაშვების ვიზუალიზაცია\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def f(x, y):\n",
        "    return x**2 + y**2\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = np.linspace(-5, 5, 100)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = f(X, Y)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.contour(X, Y, Z, levels=20)\n",
        "plt.colorbar(label='f(x, y)')\n",
        "plt.title('გრადიენტული დაშვების ვიზუალიზაცია')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "\n",
        "# გრადიენტული დაშვების სიმულაცია\n",
        "x, y = 4, 4\n",
        "lr = 0.1\n",
        "n_iter = 20\n",
        "\n",
        "for _ in range(n_iter):\n",
        "    plt.plot(x, y, 'ro')\n",
        "    grad_x = 2 * x\n",
        "    grad_y = 2 * y\n",
        "    x -= lr * grad_x\n",
        "    y -= lr * grad_y\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tptnrmrk4nMk"
      },
      "source": [
        "უკუგავრცელება არის ალგორითმი, რომელიც გამოიყენება ნეირონულ ქსელებში წონების განახლებისთვის. ეს არის გრადიენტული დაშვების ეფექტური იმპლემენტაცია მრავალშრიანი ნეირონული ქსელებისთვის.\n",
        "\n",
        "უკუგავრცელების ძირითადი იდეა მდგომარეობს იმაში, რომ გამოვთვალოთ დანაკარგის ფუნქციის გრადიენტი ქსელის ყველა პარამეტრის მიმართ, დაწყებული გამომავალი შრიდან და გაგრძელებული უკან შემავალი შრისკენ.\n",
        "\n",
        "უკუგავრცელების ალგორითმი შედგება ორი მთავარი ნაბიჯისგან:\n",
        "\n",
        "1. წინ გავრცელება (Forward Propagation): შემავალი მონაცემები გადის ქსელში და გამოითვლება პროგნოზი.\n",
        "2. უკუგავრცელება (Backward Propagation): გამოითვლება შეცდომა და ეს შეცდომა ვრცელდება უკან ქსელში წონების განახლებისთვის.\n",
        "\n",
        "მათემატიკურად, უკუგავრცელება იყენებს ჯაჭვის წესს გრადიენტების გამოსათვლელად:\n",
        "\n",
        "∂L/∂w = ∂L/∂a * ∂a/∂z * ∂z/∂w\n",
        "\n",
        "სადაც:\n",
        "* L - დანაკარგის ფუნქცია\n",
        "* w - წონა\n",
        "* a - აქტივაციის ფუნქციის გამოსავალი\n",
        "* z - წრფივი კომბინაცია (შემავალი * წონა + ცდომილება)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAjfYPhP4wkC"
      },
      "outputs": [],
      "source": [
        "# უკუგავრცელების ვიზუალიზაცია\n",
        "import networkx as nx\n",
        "\n",
        "G = nx.DiGraph()\n",
        "G.add_edges_from([\n",
        "    (\"x1\", \"h1\"), (\"x1\", \"h2\"),\n",
        "    (\"x2\", \"h1\"), (\"x2\", \"h2\"),\n",
        "    (\"h1\", \"o1\"), (\"h2\", \"o1\")\n",
        "])\n",
        "\n",
        "pos = {\"x1\": (0, 1), \"x2\": (0, 0),\n",
        "       \"h1\": (1, 0.75), \"h2\": (1, 0.25),\n",
        "       \"o1\": (2, 0.5)}\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_color='lightblue',\n",
        "        node_size=3000, arrowsize=20)\n",
        "\n",
        "plt.title(\"ნეირონული ქსელის არქიტექტურა უკუგავრცელებისთვის\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3-Zce_p443Q"
      },
      "source": [
        "# გრადიენტული დაშვების ვარიაციები\n",
        "არსებობს გრადიენტული დაშვების რამდენიმე ვარიაცია, რომლებიც გამოიყენება ოპტიმიზაციის გასაუმჯობესებლად:\n",
        "\n",
        "1. სტოქასტური გრადიენტული დაშვება (SGD): იყენებს ერთ შემთხვევით მაგალითს ყოველ იტერაციაზე.\n",
        "2. მინი-ბეტჩ გრადიენტული დაშვება: იყენებს მცირე ჯგუფს შემთხვევით მაგალითებს.\n",
        "3. მომენტუმით გრადიენტული დაშვება: ამატებს \"მომენტუმს\" ოპტიმიზაციის პროცესში.\n",
        "4. Adam: ადაპტური მომენტის შეფასება, აერთიანებს მომენტუმსა და RMSprop-ს.\n",
        "\n",
        "განვიხილოთ თითოეული შემთხვევა:\n",
        "\n",
        "**SGD** არის გრადიენტული დაშვების ვარიაცია, რომელიც იყენებს მხოლოდ ერთ შემთხვევით მაგალითს ყოველ იტერაციაზე გრადიენტის გამოსათვლელად და პარამეტრების განახლებისთვის.\n",
        "განმარტება:\n",
        "\n",
        "    θ(t+1) = θ(t) - η∇J(θ(t); x(i), y(i))\n",
        "\n",
        "რომელშიც θ(t) არის პარამეტრების ვექტორი t იტერაციაზე\n",
        "η არის სწავლების სიჩქარე\n",
        "∇J(θ(t); x(i), y(i)) არის დანაკარგის ფუნქციის გრადიენტი, გამოთვლილი ერთი შემთხვევითი მაგალითისთვის (x(i), y(i))\n",
        "მისი უპირატესობები იმით გამოიხატება, რომ არის საკმაოდ სწრაფი და რადგან სტოქასტურია მარტივად აღწევს ლოკალურ მინიმუმებს თავს. თუმცა მაღალი ვარიაციულობა აქვს რაც მის ნაკლად შეგვიძლია განვიხილოთ.\n",
        "\n",
        "**მინი-ბეტჩ გრადიენტული დაშვება** არის კომპრომისი SGD-სა და ბეტჩ გრადიენტულ დაშვებას შორის. ის იყენებს მცირე ჯგუფს (მინი-ბეტჩს) შემთხვევით მაგალითებს ყოველ იტერაციაზე.\n",
        "განმარტება:\n",
        "\n",
        "    θ(t+1) = θ(t) - η(1/m)∑∇J(θ(t); x(i:i+m), y(i:i+m))\n",
        "რომელშიც m არის მინი-ბეტჩის ზომა.\n",
        "\n",
        "    ∑∇J(θ(t); x(i:i+m), y(i:i+m)) არის გრადიენტების ჯამი მინი-ბეტჩისთვის.\n",
        "\n",
        "\n",
        "მისი უპირატესობა SGD სთან უფრო სტაბილურობაა ასევე ეფექტურია გამოთვლების თვალსაზრისით. ნაკლი ისაა, რომ უნდა შეირჩეს მინი-ბეჩის ზომა ხელით.\n",
        "\n",
        "**მომენტუმით გრადიენტული დაშვება** ამატებს \"მომენტუმის\" ტერმინს ოპტიმიზაციის პროცესში, რაც ეხმარება ალგორითმს გადალახოს ლოკალური მინიმუმები და დააჩქაროს კონვერგენცია.\n",
        "\n",
        "განმარტება:\n",
        "\n",
        "    v(t) = γv(t-1) + η∇J(θ(t))\n",
        "    θ(t+1) = θ(t) - v(t)\n",
        "\n",
        "რომელშიც v(t) არის სიჩქარის ვექტორი\n",
        "γ არის მომენტუმის კოეფიციენტი (ჩვეულებრივ 0.9)\n",
        "\n",
        "ამ პროცესის უპირატესობა არის კონვერგენციას სიჩქარე და ლოკალური მინიმუმების გადალახვა.\n",
        "\n",
        "**Adam** არის ადაპტური სწავლების სიჩქარის ოპტიმიზაციის ალგორითმი, რომელიც აერთიანებს მომენტუმისა და RMSprop-ის იდეებს.\n",
        "განმარტება:\n",
        "\n",
        "\n",
        "    m(t) = β1m(t-1) + (1-β1)∇J(θ(t))\n",
        "    v(t) = β2v(t-1) + (1-β2)(∇J(θ(t)))^2\n",
        "    m̂(t) = m(t) / (1-β1^t)\n",
        "    v̂(t) = v(t) / (1-β2^t)\n",
        "    θ(t+1) = θ(t) - η * m̂(t) / (√v̂(t) + ε)\n",
        "\n",
        "*  m(t) არის პირველი მომენტის შეფასება (გრადიენტის საშუალო)\n",
        "*  v(t) არის მეორე მომენტის შეფასება (გრადიენტის ვარიანსი)\n",
        "*  β1 და β2 არის დაშლის კოეფიციენტები (ჩვეულებრივ β1 = 0.9 და β2 = 0.999)\n",
        "*  ε არის მცირე რიცხვი ნულით გაყოფის თავიდან ასაცილებლად\n",
        "\n",
        "\n",
        "ადაპტური სწავლების სიჩქარე თითოეული პარამეტრისთვის\n",
        "კარგად მუშაობს მრავალ პრაქტიკულ პრობლემაზე\n",
        "ეფექტურია არასტაციონარული მიზნების შემთხვევაში, თუმცა შეიძლება იყოს გამოთვლითად უფრო ძვირი, ვიდრე სხვა.\n",
        "\n",
        "\n",
        "ყველა ეს მეთოდი მიზნად ისახავს გრადიენტული დაშვების გაუმჯობესებას სხვადასხვა გზით და მათი არჩევა დამოკიდებულია კონკრეტულ პრობლემაზე და მონაცემთა ნაკრებზე. პრაქტიკაში, Adam ხშირად გამოიყენება როგორც კარგი საწყისი წერტილი, მაგრამ ექსპერიმენტირება სხვადასხვა მეთოდებით შეიძლება სასარგებლო იყოს ოპტიმალური შედეგების მისაღწევად."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETsodIGA5AdS"
      },
      "source": [
        "ახლა განვიხილოთ მარტივი პრაქტიკული მაგალითი უკუგავრცელებისა და გრადიენტული დაშვების გამოყენებით"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yFwaFSS3OIW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# მონაცემების შექმნა\n",
        "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
        "\n",
        "# მოდელის განსაზღვრა\n",
        "class XORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(XORNet, self).__init__()\n",
        "        self.hidden = nn.Linear(2, 2)\n",
        "        self.output = nn.Linear(2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.hidden(x))\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "model = XORNet()\n",
        "\n",
        "# დანაკარგის ფუნქციისა და ოპტიმიზატორის განსაზღვრა\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# მოდელის ტრენირება\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "    # წინ გავრცელება\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # უკუგავრცელება და ოპტიმიზაცია\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f'ეპოქა [{epoch+1}/{epochs}], დანაკარგი: {loss.item():.4f}')\n",
        "\n",
        "# შედეგების შემოწმება\n",
        "with torch.no_grad():\n",
        "    predicted = model(X)\n",
        "    print(\"\\nსაბოლოო პროგნოზები:\")\n",
        "    print(predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkLKlbnc5Q7_"
      },
      "source": [
        "ამ მაგალითში ჩვენ გამოვიყენეთ მარტივი ნეირონული ქსელი XOR ლოგიკური ოპერაციის შესასწავლად.\n",
        "მოდელი იყენებს უკუგავრცელებას გრადიენტების გამოსათვლელად და სტოქასტურ გრადიენტულ დაშვებას (SGD) წონების განახლებისთვის."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3E13sy45H9w"
      },
      "source": [
        "# დასკვნა\n",
        "უკუგავრცელება და გრადიენტული დაშვება არის ფუნდამენტური ტექნიკები ნეირონული ქსელების ტრენირებისთვის.\n",
        "ისინი საშუალებას გვაძლევს ეფექტურად ვიპოვოთ ოპტიმალური პარამეტრები რთული, არაწრფივი ფუნქციებისთვის.\n",
        "\n",
        "ამ ტექნიკების გაგება და გამოყენება არის კრიტიკული ღრმა სწავლების სფეროში წარმატებისთვის."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THIk_Ugn5Wg4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
